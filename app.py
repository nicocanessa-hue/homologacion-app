# app.py
import streamlit as st
import pandas as pd
import re, unicodedata
from collections import defaultdict
from docx import Document

st.set_page_config(page_title="Especificaciones ¬∑ Lector DOCX Pro", page_icon="üìÑ", layout="wide")
st.title("üìÑ Lector de Especificaciones (DOCX) con encabezados multinivel")

# ----------------- helpers de texto/encabezados -----------------
def nrm(s):
    if s is None: return ""
    s = str(s).strip().lower()
    s = "".join(c for c in unicodedata.normalize("NFD", s) if unicodedata.category(c) != "Mn")
    s = re.sub(r"\s+", " ", s)
    return s

def make_unique(cols):
    seen = {}
    out = []
    for c in cols:
        c = "" if c is None else str(c).strip()
        if c in seen:
            seen[c] += 1
            out.append(f"{c}_{seen[c]}")
        else:
            seen[c] = 0
            out.append(c)
    return out

def ffill_row(row):
    cur = ""
    out = []
    for x in row:
        x = "" if x is None else str(x).strip()
        if x != "":
            cur = x
        out.append(cur)
    return out

def build_header_multirows(rows, max_header_rows=4):
    """
    Construye encabezados combinando hasta 'max_header_rows' filas superiores.
    Concatena niveles como: Top|Sub|Subsub
    """
    header_rows = rows[:max_header_rows]
    ffilled = [ffill_row(r) for r in header_rows]

    # Combina por columna
    headers = []
    for col in zip(*ffilled):
        parts = [p for p in col if p]  # solo no vac√≠os
        header = "|".join(parts) if parts else ""
        headers.append(header)

    headers = make_unique(headers)
    return headers, len(header_rows)

# ----------------- normalizaci√≥n de columnas -----------------
# Mapa a nombres finales ‚Äúbonitos‚Äù
PRETTY = {
    # F√≠sico-qu√≠micos
    "parametro": "PAR√ÅMETRO",
    "especificacion|min": "MIN",
    "especificacion|m√≠n": "MIN",
    "especificacion|minimo": "MIN",
    "especificacion|target": "TARGET",
    "especificacion|max": "MAX",
    "especificacion|m√°x": "MAX",
    "unidad": "UNIDAD",
    "metodo utilizado": "M√âTODO UTILIZADO",
    "periodicidad de control": "PERIODICIDAD DE CONTROL",
    "coa (si/no)": "CoA (S√≠/No)",

    # Microbiol√≥gicos (deja como est√° si no mapea)
    "metodo": "M√âTODO",
    "grupo": "GRUPO",
    "plan de muestreo|categoria": "PLAN DE MUESTREO|Categor√≠a",
    "plan de muestreo|clase": "PLAN DE MUESTREO|Clase",
    "plan de muestreo|n": "PLAN DE MUESTREO|n",
    "plan de muestreo|c": "PLAN DE MUESTREO|c",
    "limite|m": "L√çMITE|m",
    "limite|m.": "L√çMITE|m",
    "limite|max": "L√çMITE|M",
    "limite|m_": "L√çMITE|m",
    "limite|m mayus": "L√çMITE|M",
}

# Aliases para reconocer variantes/espa√±ol-ingl√©s
ALIASES = {
    "parametro": ["parametro", "par√°metro"],
    "especificacion": ["especificacion", "especificaci√≥n", "spec", "requisito", "valor"],
    "unidad": ["unidad", "unit", "units"],
    "metodo utilizado": ["metodo utilizado", "metodo", "m√©todo", "method"],
    "periodicidad de control": ["periodicidad de control", "frecuencia", "periodicidad"],
    "coa (si/no)": ["coa (si/no)", "coa (s√≠/no)", "coa"],

    # Micro
    "metodo": ["metodo", "m√©todo", "method"],
    "grupo": ["grupo", "group"],
    "plan de muestreo": ["plan de muestreo", "sampling plan", "plan muestreo", "plan"],
    "categoria": ["categoria", "categor√≠a", "category"],
    "clase": ["clase", "class"],
    "limite": ["limite", "l√≠mite", "limit"],
    "m": ["m"],
    "max": ["m", "m "],  # algunos documentos ponen 'M' en may√∫scula; lo resolveremos abajo
    "n": ["n"],
    "c": ["c"],
}

def canon_part(token):
    t = nrm(token)
    for key, alts in ALIASES.items():
        for a in alts:
            if nrm(a) == t:
                return key
    return t

def canon_key(header):
    """'ESPECIFICACI√ìN|M√çN' -> 'especificacion|min' (clave can√≥nica)"""
    parts = [p for p in header.split("|") if p]
    mapped = [canon_part(p) for p in parts]
    return "|".join(mapped)

def coalesce_groups(df, name_map=PRETTY):
    """
    Agrupa columnas por 'canon_key' y hace coalesce por fila (primer no vac√≠o).
    Renombra con PRETTY si aplica.
    """
    if df is None or df.empty:
        return df

    groups = defaultdict(list)
    for c in df.columns:
        groups[canon_key(c)].append(c)

    out = pd.DataFrame(index=df.index)

    for key, cols in groups.items():
        merged = df[cols].replace({"": pd.NA}).bfill(axis=1).iloc[:, 0]
        final = name_map.get(key, None)
        if final is None:
            # si es 'especificacion' sin subclave, no a√±adir (preferimos MIN/TARGET/MAX)
            if key.startswith("especificacion|"):
                sub = key.split("|")[-1].upper()
                final = sub
            else:
                final = cols[0]
        out[final] = merged.fillna("")

    # Fallback para PAR√ÅMETRO si no lo detect√≥
    if "PAR√ÅMETRO" not in out.columns:
        for c in list(out.columns):
            if nrm(c) in ("parametro", "par√°metro"):
                out.rename(columns={c: "PAR√ÅMETRO"}, inplace=True)

    # Orden sugerido si existen
    pref_order = [
        "PAR√ÅMETRO", "MIN", "TARGET", "MAX",
        "UNIDAD", "M√âTODO UTILIZADO", "M√âTODO",
        "GRUPO",
        "PLAN DE MUESTREO|Categor√≠a", "PLAN DE MUESTREO|Clase",
        "PLAN DE MUESTREO|n", "PLAN DE MUESTREO|c",
        "L√çMITE|m", "L√çMITE|M",
        "PERIODICIDAD DE CONTROL", "CoA (S√≠/No)"
    ]
    ordered = [c for c in pref_order if c in out.columns]
    rest = [c for c in out.columns if c not in ordered]
    out = out[ordered + rest]

    # Elimina filas completamente vac√≠as
    out = out[out.apply(lambda r: r.astype(str).str.strip().any(), axis=1)].reset_index(drop=True)
    return out

# ----------------- lectura de DOCX a DataFrames -----------------
def read_docx_tables(file, max_header_rows=4):
    """
    Extrae todas las tablas del .docx como DataFrames, soportando encabezados de m√∫ltiple nivel.
    """
    doc = Document(file)
    tables = []
    for t in doc.tables:
        rows = []
        for r in t.rows:
            cells = []
            for c in r.cells:
                txt = " ".join(p.text for p in c.paragraphs).strip()
                txt = " ".join(txt.split())
                cells.append(txt)
            rows.append(cells)
        if not rows:
            continue

        # normaliza ancho
        max_cols = max(len(r) for r in rows)
        rows = [r + [""] * (max_cols - len(r)) for r in rows]

        # encabezado multinivel
        header, body_start = build_header_multirows(rows, max_header_rows=max_header_rows)
        body = rows[body_start:] if len(rows) > body_start else []

        df = pd.DataFrame(body, columns=header) if body else pd.DataFrame(columns=header)
        tables.append(df)
    return tables

# ----------------- UI -----------------
docx_file = st.file_uploader("üìÇ Sube la especificaci√≥n (.docx)", type=["docx"])

if docx_file:
    try:
        raw_tables = read_docx_tables(docx_file, max_header_rows=4)
        if not raw_tables:
            st.warning("No se detectaron tablas en el documento.")
        for i, raw in enumerate(raw_tables, start=1):
            st.divider()
            st.subheader(f"üìä Tabla {i}")

            st.caption("Encabezado multinivel detectado (original)")
            st.dataframe(raw, use_container_width=True)

            st.caption("‚úÖ Normalizada / fusionada (√≥ptima para checklist)")
            clean = coalesce_groups(raw)
            st.dataframe(clean, use_container_width=True)

            st.download_button(
                f"‚¨áÔ∏è Descargar Tabla {i} normalizada (CSV)",
                data=clean.to_csv(index=False).encode("utf-8"),
                file_name=f"tabla_{i}_normalizada.csv",
                mime="text/csv",
                key=f"dl_norm_{i}"
            )

    except Exception as e:
        st.error(f"Error leyendo el DOCX: {e}")
else:
    st.info("Sube un .docx para comenzar.")
